{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWIN Transformer\n",
    "#### Architecture in torchvision==0.13.0+cu113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import headers\n",
    "Requires:\n",
    "- `torch` - PyTorch for base-handling\n",
    "- `torchvision - 0.13.0+cu113` - torchvision for getting SWIN Transformer model\n",
    "- `torch.utils.data.DataLoader` - Efficient dataset loading for training\n",
    "- `tqdm_notebook` - Progress bars for Jupyter Notebooks\n",
    "- `prefetch_generator.BackgroundGenerator` - Allows pre-fetching of next mini-batches for quicker training (requires extra RAM memory)\n",
    "- `torchmetrics` - Used for metrics and evaluation\n",
    "- `torchviz` - Used for visualizing the model architecture\n",
    "- `torch.utils.tensorboard` - Used for accessing and uploading model-training metrics to TensorBoard\n",
    "- `os` - Used for simple file-handling operations\n",
    "- `torchvision.transforms.functional` - Used for exposing torchvision.transforms as functions instead of nn.Module-compatible layers\n",
    "- `matplotlib.pyplot` - Used for plotting image-based results\n",
    "- `warnings` - Used for suppressing simple warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDGnGpXzvcgm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "!pip install torchviz torchmetrics -q\n",
    "import torchmetrics\n",
    "from torchviz import make_dot\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Defining Transformations\n",
    "### - Downloading CIFAR-100 Dataset for Training and Testing\n",
    "### - Defining DataLoader for training \n",
    "(adjust batch-size with respect to on-device GPU Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIOlZEJawg9s"
   },
   "outputs": [],
   "source": [
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(), \n",
    "    torchvision.transforms.RandomHorizontalFlip(), \n",
    "    torchvision.transforms.RandomRotation(45)\n",
    "])\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_ds = torchvision.datasets.CIFAR100(root=\"./train\", download=True, train=True, transform=train_transforms)\n",
    "test_ds = torchvision.datasets.CIFAR100(root=\"./test\", train=False, transform=test_transforms)\n",
    "\n",
    "train_load = DataLoader(train_ds, batch_size=24, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_load = DataLoader(test_ds, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading class names from dataset meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdcY7xS40-aN"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle\n",
    "tar = tarfile.open(\"./train/cifar-100-python.tar.gz\", \"r:gz\")\n",
    "member_list = tar.getmembers()\n",
    "f = tar.extractfile(member_list[4])\n",
    "content = pickle.loads(f.read())\n",
    "class_names = content['fine_label_names']\n",
    "\n",
    "def getLabel(index):\n",
    "    return class_names[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Instantiating the SWIN Transformer model from torchvision\n",
    "### - Changing the head of the model from 1000-output to 100-output `nn.Linear()` layer\n",
    "### - Checking presence of NVIDIA GPU and appropriate CUDA drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin = torchvision.models.swin_b()\n",
    "swin.head = torch.nn.Linear(1024, 100, bias=True)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Testing model by passing 1 sample image through it\n",
    "### - Visualizing model architecture \n",
    "(as the model is too huge, `make_dot` instead saves the result as a .png file for later viewing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuG6Tvvt7BJM"
   },
   "outputs": [],
   "source": [
    "img = next(iter(train_ds))[0]\n",
    "\n",
    "batch_img = img.float()\n",
    "batch_img.unsqueeze_(0)\n",
    "\n",
    "yhat = swin(batch_img)\n",
    "\n",
    "make_dot(yhat, params=dict(list(swin.named_parameters()))).render(\"swin\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tldpjZbu9wkG"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(swin.parameters(), amsgrad=True)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining helper function for returning class name from Label-encoded integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(index):\n",
    "    return class_names[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test device to check if GPU is available or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu_else_cpu():\n",
    "        devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "        return devices if devices else [torch.device('cpu')]\n",
    "device = try_gpu_else_cpu()[0]\n",
    "print(device)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function - Takes input for number of epochs to train\n",
    "- Allows setting for use of Tensorboard, but increases processing time considerably\n",
    "- Saves model weights and optimizer weights after processing of every epoch in `./swin-epochs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWKIZ892PjVK"
   },
   "outputs": [],
   "source": [
    "def fit(n_epochs, model, train_loader, optimizer, loss_fn, tensorboard=False):\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    if tensorboard:\n",
    "        writer = SummaryWriter()\n",
    "\n",
    "    CURRENT_DIRECTORY = os.getcwd()\n",
    "    EPOCH_DIRECTORY = os.path.join(CURRENT_DIRECTORY, 'swin-epochs')\n",
    "    if not os.path.exists(EPOCH_DIRECTORY):\n",
    "        os.mkdir(EPOCH_DIRECTORY)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, data in tqdm_notebook(enumerate(train_loader), 0, len(train_loader)):\n",
    "\n",
    "            img, lbl = data\n",
    "            img, lbl = img.float().to(device=device), lbl.double().to(device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y = model(img)\n",
    "            loss = loss_fn(lbl, torch.argmax(y, axis=1).double()).to(device=device)\n",
    "            loss.requires_grad = True\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if tensorboard:\n",
    "                writer.add_scalar('Loss/Train', loss, i)\n",
    "                writer.add_graph(model, img)\n",
    "\n",
    "\n",
    "        CHECKPOINT_PATH = os.path.join(EPOCH_DIRECTORY, f'model_ckpt_epoch{epoch+1}.pt')\n",
    "        torch.save({\n",
    "            \"model.state_dict\" : model.state_dict(),\n",
    "            \"optimizer.state_dict\" : optimizer.state_dict(),\n",
    "            \"epoch\":epoch\n",
    "        }, CHECKPOINT_PATH)\n",
    "    \n",
    "    if tensorboard:\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{n_epochs} completed')\n",
    "\n",
    "n_epoch = int(input(\"Enter no. of epochs to train for: \"))\n",
    "fit(n_epoch, swin, train_load, optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for prediction on test-set and calculation of Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_loader, model):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    actuals = []\n",
    "    for img, lbl in tqdm_notebook(test_loader, 0, len(test_loader)):\n",
    "        img, lbl = img.float().to(device=device), lbl.to(device=device)\n",
    "        y = model(img)\n",
    "        preds.extend(torch.argmax(y, axis=1).int().tolist())\n",
    "        actuals.extend(lbl.tolist())\n",
    "    accuracy = torchmetrics.functional.accuracy(torch.Tensor(preds).int(), torch.Tensor(actuals).int()).item()\n",
    "    print('Accuracy: ', accuracy*100, '%')\n",
    "predict(test_load, swin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to display images along with predicted and actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs, lbls, true_lb):\n",
    "    imgs = list(imgs.cpu())\n",
    "    imgs = [F.to_pil_image(i) for i in imgs]\n",
    "    lbls_list = [getLabel(i) for i in lbls.tolist()]\n",
    "    true_lb_list = [getLabel(i) for i in true_lb.tolist()]\n",
    "    fig, ax = plt.subplots((len(imgs) // 2), 2, squeeze=False, figsize=(10, 10))\n",
    "    fig.subplots_adjust(bottom = 0.15)\n",
    "    fig.tight_layout()\n",
    "    for i, img in enumerate(imgs):\n",
    "        ax[(i // 2), i%2].set_title(f\"Prediction : {lbls_list[i]}\\nActual : {true_lb_list[i]}\")\n",
    "        ax[(i // 2), i%2].imshow(img)\n",
    "        ax[(i // 2), i%2].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lbl = next(iter(test_load))\n",
    "img = img.float().cuda()\n",
    "preds = swin(img)\n",
    "lbl = lbl.cuda()\n",
    "preds = torch.argmax(preds, axis=1)\n",
    "show(img, preds, lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding thoughts:\n",
    "\n",
    "The model is fairly impaired due to the small image size of CIFAR-100 (32x32 with 3-channels) but with drop-in replacements of larger images along with better computing power enabled by GPUs, it can be much stronger and can predict much more reliably\n",
    "The model required 01:58 minutes to train for 1 epoch over 50000 samples of CIFAR-100 on a NVIDIA GTX-1650Ti Mobile 4GB GPU"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SWIN.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "oss-ml",
   "language": "python",
   "name": "oss-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
